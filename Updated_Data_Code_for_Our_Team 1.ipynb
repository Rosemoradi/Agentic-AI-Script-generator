{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is our data script. Run the cells below and download the txt files and store them in our Google Drive for model traiing.\n",
        "\n",
        "**Part A. FRIENDS scripts**\n",
        "  1. Import FRIENDS scripts from Kaggle.\n",
        "  2. Sort each episode in an order to ensure data coherence.\n",
        "  3. Add a marker for End of Episode at the end of each episode.\n",
        "  4. Split the data into train, val, test sets (ratio is approximately 70/10/20).\n",
        "  5. Remove any website addresses that might be embedded in the data.\n",
        "\n",
        "**Part B. WIKI-Text-103**\n",
        "  1. Import Wiki-text-103 dataset from Hugging Face, take only the test set, and write it into a wikitext103_test.txt file for download."
      ],
      "metadata": {
        "id": "VmUrOd1M6KjF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part A. FRIENDS scripts**"
      ],
      "metadata": {
        "id": "CkC7WedS6NXz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiPWGIJ_6Jyi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"blessondensil294/friends-tv-series-screenplay-script\")\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "B_8Qpbvq6Tg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's count how many episodes we have\n",
        "input_dir = \"/root/.cache/kagglehub/datasets/blessondensil294/friends-tv-series-screenplay-script/versions/1\"\n",
        "files = os.listdir(input_dir)\n",
        "file_count = len([f for f in os.listdir(input_dir) if f.endswith(\".txt\")])\n",
        "print(file_count)"
      ],
      "metadata": {
        "id": "pdnkl7lS6Ujy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Updated the function below to sort and then read files. This keeps all the episodes in the order (Season 1 to Season 10)."
      ],
      "metadata": {
        "id": "OR3dgaFb6X3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read and write episodes\n",
        "def create_concatenated_data(input_dir, output_file, max_files, start_index):\n",
        "\n",
        "    # read all files in directory that ends with \"\".txt\"\n",
        "    all_files = sorted([f for f in os.listdir(input_dir) if f.endswith(\".txt\")]) # in a sorted order to ensure episode coherence\n",
        "\n",
        "    # slice the list from start_index to start_index+max_files\n",
        "    sliced_files = all_files[start_index:start_index + max_files]\n",
        "\n",
        "    # open up output_file in write mode\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as out_f:\n",
        "        for i, fname in enumerate(sliced_files, start=1): # loop through every episode\n",
        "            with open(os.path.join(input_dir, fname), \"r\", encoding=\"utf-8\") as ep_f: # read every episode\n",
        "                text = ep_f.read().strip() # take its text\n",
        "            out_f.write(text + \"\\n\\n### END OF EPISODE ###\\n\\n\") # at the end of one episode, mark it for the model clearly that this is the end.\n",
        "\n",
        "    # print confirmation\n",
        "    print(f\"Taking file index {start_index} to {start_index + max_files}, a total of {len(sliced_files)} episodes, and saving them into {output_file}.\")"
      ],
      "metadata": {
        "id": "T5wE0iBl6WF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # 1) manually generate the train/val/test data files with a 70/10/20 split\n",
        "create_concatenated_data(input_dir=input_dir, output_file=\"train_data.txt\", max_files=160, start_index=0)\n",
        "create_concatenated_data(input_dir=input_dir, output_file=\"val_data.txt\", max_files=22, start_index=160)\n",
        "create_concatenated_data(input_dir=input_dir, output_file=\"test_data.txt\", max_files=46, start_index=182)"
      ],
      "metadata": {
        "id": "BWTa0A0w6o6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "During model training we observed the fine-tuned model someitmes generates website addresses out of nowhere. We thought maybe there are some website information embedded in our training data. Hence, we are using the code below to clean it off."
      ],
      "metadata": {
        "id": "WmiSVFw96r7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: We've explored a few preprocessing techniques like manually specifying rules to label character dialogues and actions, stage directions and time lapses. We also tried to use the autocorrector library to check for spelling issues and typos. But all create more problems than they solve (like labeling the wrong context, or auto-correcting a short form of character name into something else, like \"Rach\" was supposed to mean \"Rachel\" but it got corrected into \"each\"). So in the end we just kept the website cleaning function."
      ],
      "metadata": {
        "id": "pTC3vshu6tSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clean websites\n",
        "def clean_websites(input_file, output_file):\n",
        "    # URL patterns to get rid of\n",
        "    url_patterns = [r'https?://\\S+',\n",
        "                    r'www\\.\\S+',\n",
        "                    r'\\b\\w+\\.(com|org|net|edu|gov|io|co)\\S*']\n",
        "    # combine patterns\n",
        "    pattern = re.compile('|'.join(url_patterns), re.IGNORECASE)\n",
        "    try:\n",
        "        # read input file\n",
        "        with open(input_file, 'r', encoding='utf-8') as infile:\n",
        "            text = infile.read()\n",
        "        # remove URLs\n",
        "        cleaned_text = pattern.sub('', text).strip()\n",
        "        # write cleaned text to output file\n",
        "        with open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "            outfile.write(cleaned_text)\n",
        "        print(f\"Cleaned text has been written to {output_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# run the function for all the data splits\n",
        "clean_websites('train_data.txt', 'cleaned_train_data.txt')\n",
        "clean_websites('val_data.txt', 'cleaned_val_data.txt')\n",
        "clean_websites('test_data.txt', 'cleaned_test_data.txt')"
      ],
      "metadata": {
        "id": "b0EmtsR96s0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part B. Download WikiText103**"
      ],
      "metadata": {
        "id": "jG3kw4U06yUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "1lQFmKyo6yns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# take only the test set\n",
        "ds = load_dataset(\"Salesforce/wikitext\", \"wikitext-103-v1\", split=\"test\")\n",
        "\n",
        "# write it into a .txt file for download\n",
        "with open(\"wikitext103_test.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "  for example in ds:\n",
        "    text = example[\"text\"]\n",
        "    f.write(text.strip() + \"\\n\")"
      ],
      "metadata": {
        "id": "u4AO2XeS60_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After running this code we should have 7 .txt files on Colab. We can save the last 4 .txt files on our Google Drive and call them in the Training Script."
      ],
      "metadata": {
        "id": "VqqWFvtr63Et"
      }
    }
  ]
}