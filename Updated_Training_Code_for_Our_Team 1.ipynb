{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. Import**"
      ],
      "metadata": {
        "id": "Pwty3G7j42Ue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with imports and installing Hugging Face datasets."
      ],
      "metadata": {
        "id": "PBmRFIiA47lY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYd4Q0s14uEa"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling, AutoModelForCausalLM\n",
        "from datasets import load_dataset, concatenate_datasets, Dataset, DatasetDict\n",
        "from peft import LoraConfig, get_peft_model, TaskType"
      ],
      "metadata": {
        "id": "rTfBhZRH48qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Tokenization and Data Blocks**"
      ],
      "metadata": {
        "id": "1BpARx0A5AS4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's access our .txt files from Google Drive and prepare them with the tokenzier.\n",
        "\n",
        "*   Step 1. Read all files as a concatenated text and Tokenize everything.\n",
        "*   Step 2. Create blocks of data from the one long piece of tokens. (For GPT-2 models the block size is pretty limited due to their small 1024 token context window)\n",
        "*   Step 3. Make Hugging Face dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "Bl1PO8XK5CqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# set up tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "axMfzmxT4-Dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------Saber's path--------------\n",
        "\n",
        "# read and tokenize each file\n",
        "def load_and_tokenize(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "    tokenized = tokenizer(text)\n",
        "    return tokenized['input_ids']\n",
        "\n",
        "# specify file path in Google Drive\n",
        "data_path = \"/content/drive/MyDrive/SEP775NLP_Final_Project/Friends_script/\"\n",
        "\n",
        "# run the function above to tokenize the data\n",
        "train_ids = load_and_tokenize(f\"{data_path}/cleaned_train_data.txt\") # used latest version of processed data here\n",
        "val_ids = load_and_tokenize(f\"{data_path}/cleaned_val_data.txt\")\n",
        "test_ids = load_and_tokenize(f\"{data_path}/cleaned_test_data.txt\")\n",
        "general_test_ids = load_and_tokenize(f\"{data_path}/wikitext103_test.txt\")"
      ],
      "metadata": {
        "id": "nBlbFKPc5Ftu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create blocks of sequences\n",
        "block_size=512\n",
        "\n",
        "def create_blocks(all_tokens, block_size):\n",
        "  blocks = []\n",
        "  # loop through all of the tokens, split them by block_size\n",
        "  for i in range(0, len(all_tokens), block_size):\n",
        "    tokens_in_this_block = all_tokens[i:i+block_size] # takes [0:512], then [512:1024]...\n",
        "    blocks.append(tokens_in_this_block)\n",
        "  return blocks # returns a list of list of blocks\n",
        "\n",
        "# run the function above to create the data blocks\n",
        "train_blocks = create_blocks(train_ids, block_size)\n",
        "val_blocks = create_blocks(val_ids, block_size)\n",
        "test_blocks = create_blocks(test_ids, block_size)\n",
        "general_test_blocks = create_blocks(general_test_ids, block_size)\n",
        "\n",
        "# verify that one block contains multiple lines of the script\n",
        "print(train_blocks[0])"
      ],
      "metadata": {
        "id": "2G5RP3kK5Kgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make dictionaries and then turn them into hugging face datasets\n",
        "train_dataset = Dataset.from_dict({\"input_ids\": train_blocks})\n",
        "val_dataset = Dataset.from_dict({\"input_ids\": val_blocks})\n",
        "test_dataset = Dataset.from_dict({\"input_ids\": test_blocks})\n",
        "general_test_dataset = Dataset.from_dict({\"input_ids\": general_test_blocks})\n",
        "\n",
        "# create the Hugging Face dataset dict\n",
        "dataset_dict = DatasetDict({\"train\": train_dataset, \"validation\": val_dataset, \"test\": test_dataset,\n",
        "                            \"general_test\": general_test_dataset})"
      ],
      "metadata": {
        "id": "EO_52rNY5LCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up data collator\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)  # For causal LM"
      ],
      "metadata": {
        "id": "4_pVGqZz5MQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Model Training**"
      ],
      "metadata": {
        "id": "mm21gjtD5Oz5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's set up the parameters for training here. We will also benchmark the pre-trained model's ability on a general Wiki language dataset (before fine-tuning). We will test the model again on this dataset after fine-tuning to see if our model retains the general language capability."
      ],
      "metadata": {
        "id": "wYYok4Ff5PH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# access the model from Hugging Face\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "# --------------------------------------------------------------------------------(comment this section out if running without LoRA)\n",
        "# freeze all the weights\n",
        "#for param in model.parameters():\n",
        "    #param.requires_grad = False\n",
        "# -------------------------------------------------------------------------------- (this code might not be needed even when running LoRa, but just in case)"
      ],
      "metadata": {
        "id": "CBF6JEA35Raj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up dummy trainer to benchmark the pre-trained model's capability on the general wiki test set\n",
        "dummy_training_args = TrainingArguments(output_dir=\"./eval_output\", do_train=False, per_device_eval_batch_size=4, report_to=\"none\")\n",
        "dummy_trainer = Trainer(model=model, args=dummy_training_args, eval_dataset=dataset_dict[\"general_test\"], data_collator=data_collator)\n",
        "\n",
        "# test the pre-trained model\n",
        "benchmark_general_eval_results = dummy_trainer.evaluate(dataset_dict[\"general_test\"])\n",
        "print(f\"General Wiki Data before Fine-Tuning: eval_loss = {benchmark_general_eval_results['eval_loss']:.2f}\")"
      ],
      "metadata": {
        "id": "ykQ3EaVb5S4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take a look at how many steps will be one epoch\n",
        "num_of_train_blocks=(len(train_blocks))\n",
        "step_per_epoch = num_of_train_blocks // 4 # note 4 here is the batch_size\n",
        "print(step_per_epoch)"
      ],
      "metadata": {
        "id": "ZX57nKia5UEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------------------------(comment this section out if running without LoRA)\n",
        "# setup LoRA configuration and wrap the model with it\n",
        "#lora_config = LoraConfig(r=32, lora_alpha=32, lora_dropout=0.05, bias=\"none\", task_type=TaskType.CAUSAL_LM) # increase r for a more powerful adapter\n",
        "#model = get_peft_model(model, lora_config)\n",
        "#model.print_trainable_parameters()# verify we're only training LoRA weights\n",
        "# --------------------------------------------------------------------------------\n",
        "\n",
        "# set up training_args\n",
        "training_args = TrainingArguments(output_dir=\"/content/drive/MyDrive/SEP775NLP_Final_Project/Models/gpt2-xl\",\n",
        "                                  evaluation_strategy=\"steps\",\n",
        "                                  save_strategy=\"steps\",\n",
        "                                  max_steps=2000,\n",
        "                                  eval_steps=400,\n",
        "                                  save_steps=400,\n",
        "                                  logging_steps=400,\n",
        "                                  #num_train_epochs=3,\n",
        "                                  per_device_train_batch_size=4,\n",
        "                                  per_device_eval_batch_size=4,\n",
        "                                  learning_rate=8e-5,\n",
        "                                  #save_total_limit=1,\n",
        "                                  resume_from_checkpoint=True,\n",
        "                                  load_best_model_at_end=True,\n",
        "                                  metric_for_best_model=\"eval_loss\",\n",
        "                                  weight_decay=0.01, # added regularization\n",
        "                                  warmup_steps=5, # added lr scheduler and warmup\n",
        "                                  lr_scheduler_type=\"linear\",\n",
        "                                  report_to=\"none\" # disable wandb\n",
        "                                  ) # default optimizer is adamw\n",
        "\n",
        "# setup trainer\n",
        "trainer = Trainer(model=model,\n",
        "                  args=training_args,\n",
        "                  train_dataset=dataset_dict[\"train\"],\n",
        "                  eval_dataset=dataset_dict[\"validation\"],\n",
        "                  data_collator=data_collator)"
      ],
      "metadata": {
        "id": "v_j_GbVY5V2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# start training\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "iQ2qcEaV5ZyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Evaluation**"
      ],
      "metadata": {
        "id": "-YcXqeiQ5aNU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take the trained model, and evaluate its performance on the two different tests sets.\n",
        "\n",
        "*   Test 1. Unseen FRIENDS scripts - to test model's ability to pick up the specific style of the show.\n",
        "*   Test 2. General Wiki English data - to test model's general language capabilities. (compare to benchmark)\n",
        "\n"
      ],
      "metadata": {
        "id": "RP1Vib165b8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation process\n",
        "eval_results = trainer.evaluate(dataset_dict[\"test\"])\n",
        "general_eval_results = trainer.evaluate(dataset_dict[\"general_test\"])\n",
        "\n",
        "print(f\"Unseen FRIENDS scripts: eval_loss = {eval_results['eval_loss']:.2f}\")\n",
        "print(f\"General Wiki Data after Fine-Tuning: eval_loss = {general_eval_results['eval_loss']:.2f}\")"
      ],
      "metadata": {
        "id": "XlBc7q4e5be6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Generation**"
      ],
      "metadata": {
        "id": "TKGchg3s5fwC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we'll use this model to generate a new episode to see for ourselves how the model performs. We trialed the following different approaches for text generation and found that better results can be obtained with the last approach, so in this version of the code we only kept that approach.\n",
        "\n",
        "*   Approach 1. Greedy Search (using argmax)\n",
        "*   Approach 2. MultiNomial selection\n",
        "*   Approach 3. MultiNomial selection with top_k and top_p filters\n",
        "\n",
        "Updated the code not to end episode until certain amount of tokens have been generated. Also updated the code to stop after hitting the end of an episode.\n",
        "\n"
      ],
      "metadata": {
        "id": "e9if1K8G5hU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create initial prompt and tokenize it\n",
        "prompt = \"\"\"\n",
        "The One with students from McMaster University\n",
        "Written by: GPT2-Base\n",
        "\n",
        "[Scene: Monica's Apartment. Monica, Ross, Rachel, and Chandler are there.]\n",
        "\n",
        "Monica: Hey guys! Big news—I heard there's going to be a party at McMaster University.\n",
        "\n",
        "Ross: A party?\n",
        "\n",
        "Rachel: Oh yeah! The students are celebrating their SEP775 class with the professors!\n",
        "\n",
        "Chandler: We should join them! I mean, where is the fun in a party without your favorite FRIENDS characters?\n",
        "\n",
        "(They all leave for the party.)\n",
        "\n",
        "[Scene: SEP775 classroom at McMaster University. Rachel, Chandler, Ross, Monica, and Phoebe are hanging out with the students.]\n",
        "\n",
        "Rachel: Wow, this place kicks ass! Look, isn’t that Professor Mahyar who teaches Natural Language Processing?\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "model_inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=True).to(model.device)\n",
        "model.generation_config.pad_token_id = tokenizer.pad_token_id # suppress padding warning\n",
        "\n",
        "# create a list that holds the prompt, future tokens will be appended into this list later on\n",
        "all_tokens = model_inputs[\"input_ids\"][0].tolist()\n",
        "\n",
        "# to avoid hitting memory limits, generate only one batch with the initial prompt, take last few tokens out to generate the next batch.\n",
        "context_window = 700 # this is the sliding window - take this many tokens to set up next batch's generation\n",
        "max_token = 2500\n",
        "step_size = 128 # generate this many tokens at a time\n",
        "num_of_loops = max_token // step_size\n",
        "\n",
        "# end episode management\n",
        "min_tokens_before_end = 2000 # generate this many tokens before being allowed to generate the word \"end\"\n",
        "end_id = tokenizer.encode(\"end\", add_special_tokens=False) # token_id of the word \"end\" (I checked it previously and it is only 1 token id, meaning it is the standalone word \"end\")\n",
        "end_of_episode_marker = \"### END OF EPISODE ###\" # this marker was added during data pre-processing. it should come right after the \"end\" word\n",
        "\n",
        "# generation loop\n",
        "for i in range(num_of_loops):\n",
        "\n",
        "  # suppresses the word \"end\" before reaching certain number of tokens\n",
        "  if len(all_tokens) < min_tokens_before_end:\n",
        "    bad_words = [end_id]\n",
        "  else:\n",
        "    bad_words = None\n",
        "\n",
        "  output = model.generate(**model_inputs,\n",
        "                          max_new_tokens=step_size,\n",
        "                          do_sample=True,\n",
        "                          top_k=40, # keep k highest probable options\n",
        "                          top_p=0.75, # keep only the options that add up to p% probability, filter out the remaining less probable options\n",
        "                          temperature=1, # to add a little bit more creativity if needed\n",
        "                          repetition_penalty=1.2,\n",
        "                          bad_words_ids=bad_words # bad words to generate before hitting certain number of tokens\n",
        "                          )\n",
        "\n",
        "  output_tokens = output[0].tolist()\n",
        "  new_tokens = output_tokens[-step_size:] # get rid of the previous context and keep only new tokens\n",
        "\n",
        "  new_text = tokenizer.decode(new_tokens, skip_special_tokens=True) # turn tokens to text to check if end marker has been generated\n",
        "  start_index_of_end_marker = new_text.find(end_of_episode_marker) # returns -1 if the marker is not found, otherwise returns the start index\n",
        "\n",
        "  if start_index_of_end_marker != -1: # if end marker is present\n",
        "    truncated_new_text = new_text[: start_index_of_end_marker + len(end_of_episode_marker)] # get rid of what is after the end marker\n",
        "    truncated_new_tokens = tokenizer.encode(truncated_new_text, add_special_tokens=False) # turn them into tokens\n",
        "    all_tokens.extend(truncated_new_tokens) # and put them in the list\n",
        "    break\n",
        "\n",
        "  else:\n",
        "      all_tokens.extend(new_tokens) # otherwise place the whole thing into the list without truncating\n",
        "\n",
        "  previous_context = output_tokens[-context_window:] # set new previous_context for next loop\n",
        "  previous_context_tensor = torch.tensor([previous_context]).to(model.device) # prepare the inputs and attention mask for next loop\n",
        "  attention_mask = (previous_context_tensor != tokenizer.pad_token_id).long()\n",
        "  model_inputs = {\"input_ids\": previous_context_tensor, \"attention_mask\":attention_mask}\n",
        "\n",
        "# decode from the list and print results\n",
        "generated_script = tokenizer.decode(all_tokens, skip_special_tokens=True)\n",
        "print(generated_script)"
      ],
      "metadata": {
        "id": "NbUOEdgS5eiv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}